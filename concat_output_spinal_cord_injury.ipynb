{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Baysor outputs (outer join)\n",
    "\n",
    "This notebook reads all per-sample outputs from a directory and concatenates them into one AnnData with an outer join on variables.\n",
    "\n",
    "**Note on memory:** concatenation creates a single in-memory AnnData. If you run out of RAM, consider writing a subset first, or using Zarr-based workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57ba835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dir: /Volumes/processing2/output_spinal_cord_injury\n",
      "Output path: /Volumes/processing2/output_spinal_cord_injury/spinal_cord_injury_merged_outer.h5ad\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "# ---- Parameters ----\n",
    "INPUT_DIR = Path('/Volumes/processing2/output_spinal_cord_injury')\n",
    "PATTERN = '**/*.h5ad'  # adjust if needed\n",
    "OUTPUT_PATH = INPUT_DIR / 'spinal_cord_injury_merged_outer.h5ad'\n",
    "\n",
    "# For reproducibility in concat\n",
    "OBS_KEY = 'batch'  # name for source file label in .obs\n",
    "\n",
    "print('Input dir:', INPUT_DIR)\n",
    "print('Output path:', OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ede9664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 132 files before filtering\n",
      "Using 66 files after filtering\n",
      "/Volumes/processing2/output_spinal_cord_injury/Data_AJ__Slide1__m50_s4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Data_AJ__Slide3__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide11__region0__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide11__region1__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide11__region2__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide11__region3__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide12__region0__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide12__region1__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide12__region2__m_50_s_4.h5ad\n",
      "/Volumes/processing2/output_spinal_cord_injury/Slide12__region3__m_50_s_4.h5ad\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Discover files\n",
    "# Filters help remove hidden files and obvious duplicates.\n",
    "EXCLUDE_HIDDEN = True\n",
    "EXCLUDE_DIRS = {'.ipynb_checkpoints'}\n",
    "EXCLUDE_SUFFIXES = {'.tmp', '.bak'}\n",
    "\n",
    "all_files = sorted(INPUT_DIR.glob(PATTERN))\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(f'No files found for pattern: {PATTERN}')\n",
    "\n",
    "def is_hidden(path: Path) -> bool:\n",
    "    return any(part.startswith('.') for part in path.parts)\n",
    "\n",
    "filtered = []\n",
    "for f in all_files:\n",
    "    if EXCLUDE_HIDDEN and is_hidden(f):\n",
    "        continue\n",
    "    if any(part in EXCLUDE_DIRS for part in f.parts):\n",
    "        continue\n",
    "    if f.suffix in EXCLUDE_SUFFIXES:\n",
    "        continue\n",
    "    filtered.append(f)\n",
    "\n",
    "# Deduplicate by resolved path (just in case)\n",
    "seen = set()\n",
    "files = []\n",
    "for f in filtered:\n",
    "    rp = f.resolve()\n",
    "    if rp in seen:\n",
    "        continue\n",
    "    seen.add(rp)\n",
    "    files.append(f)\n",
    "\n",
    "print(f'Found {len(all_files)} files before filtering')\n",
    "print(f'Using {len(files)} files after filtering')\n",
    "for f in files[:10]:\n",
    "    print(f)\n",
    "if len(files) > 10:\n",
    "    print('...')\n",
    "\n",
    "# Optional: flag duplicate stems (same filename in different dirs)\n",
    "from collections import Counter\n",
    "stem_counts = Counter(f.stem for f in files)\n",
    "dup_stems = [s for s, c in stem_counts.items() if c > 1]\n",
    "if dup_stems:\n",
    "    print('Duplicate stems found (same filename in multiple dirs):')\n",
    "    for s in dup_stems:\n",
    "        print('  ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3205800 × 500\n",
       "    obs: 'x', 'y', 'z', 'cluster', 'n_transcripts', 'density', 'elongation', 'area', 'avg_confidence', 'avg_assignment_confidence', 'max_cluster_frac', 'lifespan', 'sample', 'source_dir', 'batch'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and concatenate\n",
    "# If memory is tight, reduce the file set or try concatenating in smaller batches.\n",
    "adatas = []\n",
    "for f in files:\n",
    "    adata = sc.read_h5ad(f)\n",
    "    # Track origin for downstream grouping\n",
    "    adata.obs[OBS_KEY] = f.stem\n",
    "    adatas.append(adata)\n",
    "\n",
    "adata_merged = ad.concat(\n",
    "    adatas,\n",
    "    join='outer',\n",
    "    label=OBS_KEY,\n",
    "    keys=[f.stem for f in files],\n",
    "    index_unique='-'\n",
    ")\n",
    "\n",
    "adata_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Volumes/processing2/output_spinal_cord_injury/spinal_cord_injury_merged_outer.h5ad\n"
     ]
    }
   ],
   "source": [
    "# Write output (compressed)\n",
    "adata_merged.write_h5ad(OUTPUT_PATH, compression='gzip')\n",
    "print('Wrote:', OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db42b38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(nan)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_merged.X.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670bf630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 69., 257., 116., ...,  86.,  28.,  18.], shape=(3205800,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_merged.X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3109ece1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m adata_merged.X = \u001b[43madata_merged\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "adata_merged.X = adata_merged.X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2efa95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adata_merged.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5182c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0851baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd3dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_merged.X =X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8088c7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(55.64957982406887)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_merged.X.sum(axis = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4998cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_merged.write_h5ad(OUTPUT_PATH, compression='gzip')\n",
    "print('Wrote:', OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe27b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
